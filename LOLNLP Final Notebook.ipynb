{
 "metadata": {
  "name": "",
  "signature": "sha256:fa5fd9c33503fec255c19c603ea5fb4589cf4ddcacc18376a701ff06b263166c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#LOLNLP GIF Recommender\n",
      "Arvind Narayanan, Tom Silver, Ved Topkar\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this project, we set out to create a GIF recommender that can recommend relavent GIFs from an input piece of text. We realized that the corpus of GIF \"listicles\" on BuzzFeed made for the perfect training set of text associated with a large number of GIFs. We also realized that classifying a GIF with a specific emotion or range of emotions is a task that cannot be done very well by machine learning algorithms alone. Consequently, we split this project up into the following parts:\n",
      "\n",
      "1. [Web Scraping](#web-scraping)\n",
      "    1. [Crawling BuzzFeed for GIF Listicles](#crawling-buzzfeed)\n",
      "    2. [Scraping Articles for GIFs and Tags](#scraping-buzzfeed)\n",
      "    3. Scraping Giphy for supplemental GIFs\n",
      "2. Crowdsourced GIF Sentiment Tagging\n",
      "    1. Building the Crowdsourcing Site\n",
      "    2. GIF Image Analysis\n",
      "3. Natural Language Processing\n",
      "    1. Parts of Speech Tagging\n",
      "    2. De Novo Sentiment Analysis\n",
      "    3. Dictionary-Based Sentiment Analysis\n",
      "    4. Content Analysis\n",
      "4. Recommendation Engine"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"web-scraping\"></a>\n",
      "# Web Scraping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"crawling-buzzfeed\"></a>\n",
      "## Crawling BuzzFeed\n",
      "\n",
      "The first step in this process is for us to crawl BuzzFeed and collect a large list of URLs that correspond to GIF listicles in the format [TOM FILL THIS IN HERE]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import requests\n",
      "import Queue\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "srcs = [\"shawnkathleen/flying-this-holiday-season-dont-be-one-of-these-iakw\"]\n",
      "\n",
      "def diff(a, b):\n",
      "    \"\"\"Return the different between two lists.\"\"\"\n",
      "    b = set(b)\n",
      "    return [c for c in a if c not in b]\n",
      "\n",
      "def parse_href(url_string):\n",
      "    \"\"\"Parse the URL from an HTML string containing one. If no URL, return False.\"\"\"\n",
      "    href_idx = url_string.find('href=\"')\n",
      "    \n",
      "    if href_idx <= 0:\n",
      "        return False\n",
      "    \n",
      "    partial_string = url_string[href_idx+7:]\n",
      "    return partial_string[0:partial_string.find('\"')]\n",
      "\n",
      "def is_gif_article(gif_article):\n",
      "    \"\"\"Return True if the article is a listicle containing GIFs, False otherwise.\"\"\"\n",
      "    \n",
      "    buzzfeed_article = requests.get(\"http://www.buzzfeed.com/\"+gif_article)\n",
      "    listicle_items = buzzfeed_article.content.split('class=\"buzz_superlist_item')\n",
      "    \n",
      "    for item in listicle_items[1:-1]:\n",
      "        # Check if item contains a GIF.\n",
      "        if item.find('.gif') > 0:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "def find_other_gif_articles(gif_article):\n",
      "    \"\"\"Search 'related articles' listed on input article for possible GIFs.\"\"\"\n",
      "    gif_articles = []\n",
      "    \n",
      "    buzzfeed_article = requests.get(\"http://www.buzzfeed.com/\"+gif_article)\n",
      "    related_strings = buzzfeed_article.content.split('rel:gt_act=\"related-link/name\"')\n",
      "\n",
      "    for related_string in related_strings[1:]:\n",
      "        href = parse_href(related_string)\n",
      "        if is_gif_article(href) > 0:\n",
      "            gif_articles.append(href)\n",
      "            \n",
      "    return gif_articles\n",
      "\n",
      "def crawl_for_gif_pages(source, limit=1000, delay=0.5):\n",
      "    \"\"\"Crawl Buzzfeed, starting at the source, collecting pages that contain GIFs.\"\"\"\n",
      "    found = []\n",
      "\n",
      "    sources = []\n",
      "    sources.append(source)\n",
      "\n",
      "    while (len(found)<limit and len(sources)>0):\n",
      "        source = sources.pop()\n",
      "\n",
      "        new_article_candidates = find_other_gif_articles(source)\n",
      "        new_articles = diff(new_article_candidates, found)\n",
      "\n",
      "        found.extend(new_articles)\n",
      "        sources.extend(new_articles)\n",
      "        \n",
      "        time.sleep(delay)\n",
      "    \n",
      "    return pd.Series(found)\n",
      "\n",
      "# Crawl all of the sources\n",
      "gif_pages = crawl_for_gif_pages(srcs)\n",
      "gif_pages.to_csv('data/buzzfeed_gif_links.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Display all of the GIF\n",
      "gif_pages = pd.read_csv('data/buzzfeed_gif_links.txt')\n",
      "print gif_pages.shape\n",
      "gif_pages.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(203, 1)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>simplysinova/25-mouthwatering-dessert-gifs-k808</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> giulsi/20-amazing-will-ferrells-gifs-for-any-o...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> erinf45e036d0f/34-david-bowie-gifs-for-all-occ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>   simplysinova/25-mouthwatering-dessert-gifs-k808</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> giulsi/20-amazing-will-ferrells-gifs-for-any-o...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> erinf45e036d0f/34-david-bowie-gifs-for-all-occ...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "     simplysinova/25-mouthwatering-dessert-gifs-k808\n",
        "0  giulsi/20-amazing-will-ferrells-gifs-for-any-o...\n",
        "1  erinf45e036d0f/34-david-bowie-gifs-for-all-occ...\n",
        "2    simplysinova/25-mouthwatering-dessert-gifs-k808\n",
        "3  giulsi/20-amazing-will-ferrells-gifs-for-any-o...\n",
        "4  erinf45e036d0f/34-david-bowie-gifs-for-all-occ..."
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"scraping-buzzfeed\"></a>\n",
      "## Scraping BuzzFeed\n",
      "\n",
      "Once we have a list of a large number of BuzzFeed GIF articles, we scraped each of them individually for the article title, each GIF, and each GIF's associated text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urlfile = open('data/buzzfeed_gif_links.txt','r')\n",
      "pages = {}\n",
      "for line in urlfile:\n",
      "    line = 'http://www.buzzfeed.com/' + line.replace('\\n','')\n",
      "    if not(line in pages):\n",
      "        pages[line] = 1\n",
      "urlfile_new = open('data/buzzfeed_gif_links_pt2.txt','r')\n",
      "for line in urlfile_new:\n",
      "    line = 'http://www.buzzfeed.com/' + line.replace('\\n','')\n",
      "    if not(line in pages):\n",
      "        pages[line] = 1\n",
      "gifs = []\n",
      "for page in pages:\n",
      "    gifs.append(newscrapepage(page))\n",
      "bf = [item for sublist in gifs for item in sublist]\n",
      "df = pd.DataFrame(bf)\n",
      "df.to_csv('data/buzzfeed_gifs_final.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "buzzfeed_gifs = pd.read_csv('data/buzzfeed_gifs_final.csv',encoding='utf-8')\n",
      "buzzfeed_gifs.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0</th>\n",
        "      <th>Text</th>\n",
        "      <th>Title</th>\n",
        "      <th>URL</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1. Because I just wasn\u2019t one of those people w...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 2. Because there are SO many options to choose...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2</td>\n",
        "      <td> 3. Some days you hate every subject in the wor...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 3</td>\n",
        "      <td> 4. And other days you\u2019re super ambitious and t...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 4</td>\n",
        "      <td> 5. But either way you know that you don\u2019t know...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "   Unnamed: 0                                               Text  \\\n",
        "0           0  1. Because I just wasn\u2019t one of those people w...   \n",
        "1           1  2. Because there are SO many options to choose...   \n",
        "2           2  3. Some days you hate every subject in the wor...   \n",
        "3           3  4. And other days you\u2019re super ambitious and t...   \n",
        "4           4  5. But either way you know that you don\u2019t know...   \n",
        "\n",
        "                                               Title  \\\n",
        "0  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "1  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "2  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "3  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "4  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "\n",
        "                                                 URL  \n",
        "0  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "1  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "2  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "3  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "4  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  "
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scraping Giphy\n",
      "\n",
      "In case we needed more GIF data, we also built a simple GIF scraper for giphy, a website where people can upload and tag GIFs.\n",
      "\n",
      "Although Giphy tries to keep people from being able to scrape their site, after some exploring we found a trick in their URL encoding that allowed us to query each of the emotion tags in their tag repository and iterate through each page of GIFs.\n",
      "\n",
      "Below, we see the scraper that iterates through each of these emotions and pages and collects about 40,000 GIFs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfs = []\n",
      "emotions = ['angry', 'bored', 'disappointed', 'drunk', 'embarassed', \n",
      "           'excited', 'frustrated', 'happy', 'hungry', 'inspired', \n",
      "           'lonely', 'love', 'nervous', 'pain', 'reaction', 'relaxed', \n",
      "           'sad', 'sassy', 'scared', 'shocked', 'sick', 'stressed', \n",
      "           'surprised', 'suspicious', 'tired', 'unimpressed']\n",
      "\n",
      "for emotion in emotions:\n",
      "    print emotion\n",
      "    urls = ['http://giphy.com/search/' + emotion + '/' + str(x) +'?is=2' for x in range(100)]\n",
      "    for url in urls:\n",
      "        try:\n",
      "            page = urllib2.urlopen(url).read()\n",
      "            soup = BeautifulSoup(page)\n",
      "            images = soup.findAll('div', class_='hoverable-gif')\n",
      "\n",
      "            data = []\n",
      "            for image in images:\n",
      "                tags = [x.text.split('#')[1] for x in image.span.findAll('a')]\n",
      "                url = image.a.figure.img['src'].replace('_s', '')\n",
      "                dfs.append({'tags':tags, 'url':url, 'emotion': emotion})\n",
      "        except:\n",
      "            print 'Done with ' + emotion\n",
      "            break\n",
      "    \n",
      "data = pd.DataFrame(dfs)\n",
      "data.to_csv('data/giphy_emotions_gifs.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "giphy_gifs = pd.read_csv('data/giphy_emotions_gifs.csv')\n",
      "print giphy_gifs.shape\n",
      "giphy_gifs.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(39975, 4)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0</th>\n",
        "      <th>emotion</th>\n",
        "      <th>tags</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> angry</td>\n",
        "      <td>           [u'angry', u'horse', u'maximus']</td>\n",
        "      <td> http://media2.giphy.com/media/dOZj6qlD8Kwta/20...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> angry</td>\n",
        "      <td>                [u'angry', u'taylor swift']</td>\n",
        "      <td> http://media0.giphy.com/media/ORKQqVjegOvzq/20...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2</td>\n",
        "      <td> angry</td>\n",
        "      <td>          [u'angry', u'coach', u'throwing']</td>\n",
        "      <td> http://media2.giphy.com/media/110gOs75GuUWyY/2...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 3</td>\n",
        "      <td> angry</td>\n",
        "      <td>                  [u'angry', u'frustrated']</td>\n",
        "      <td> http://media0.giphy.com/media/yI9YaxO7OCz5K/20...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 4</td>\n",
        "      <td> angry</td>\n",
        "      <td> [u'angry', u'mario', u'nintendo', u'zoom']</td>\n",
        "      <td> http://media0.giphy.com/media/kLEuIxZJh8VG0/20...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "   Unnamed: 0 emotion                                        tags  \\\n",
        "0           0   angry            [u'angry', u'horse', u'maximus']   \n",
        "1           1   angry                 [u'angry', u'taylor swift']   \n",
        "2           2   angry           [u'angry', u'coach', u'throwing']   \n",
        "3           3   angry                   [u'angry', u'frustrated']   \n",
        "4           4   angry  [u'angry', u'mario', u'nintendo', u'zoom']   \n",
        "\n",
        "                                                 url  \n",
        "0  http://media2.giphy.com/media/dOZj6qlD8Kwta/20...  \n",
        "1  http://media0.giphy.com/media/ORKQqVjegOvzq/20...  \n",
        "2  http://media2.giphy.com/media/110gOs75GuUWyY/2...  \n",
        "3  http://media0.giphy.com/media/yI9YaxO7OCz5K/20...  \n",
        "4  http://media0.giphy.com/media/kLEuIxZJh8VG0/20...  "
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}