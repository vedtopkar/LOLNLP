{
 "metadata": {
  "name": "",
  "signature": "sha256:101355f6b41ca4419256940cc061a50fd18af4070cfd4c7fb933d1772f325825"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#LOLNLP GIF Recommender\n",
      "Arvind Narayanan, Tom Silver, Ved Topkar\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this project, we set out to create a GIF recommender that can recommend relavent GIFs from an input piece of text. We realized that the corpus of GIF \"listicles\" on BuzzFeed made for the perfect training set of text associated with a large number of GIFs. We also realized that classifying a GIF with a specific emotion or range of emotions is a task that cannot be done very well by machine learning algorithms alone. Consequently, we split this project up into the following parts:\n",
      "\n",
      "1. [Web Scraping and Crowdsourced GIF Sentiment Tagging](#web-scraping)\n",
      "    1. [Crawling BuzzFeed for GIF Listicles](#crawling-buzzfeed)\n",
      "    2. [Scraping Articles for GIFs and Tags](#scraping-buzzfeed)\n",
      "    3. [Scraping Giphy for supplemental GIFs](#scraping-giphy)\n",
      "    4. [Crowdsourcing Sentiment Tagging](#crowdsourced-tagging)\n",
      "2. Natural Language Processing and Image Analysis\n",
      "    1. Image color analysis\n",
      "    1. Image motion analysis\n",
      "    1. Image luminance analysis\n",
      "    1. Parts of Speech Tagging\n",
      "    2. De Novo Sentiment Analysis\n",
      "    3. Dictionary-Based Sentiment Analysis\n",
      "    4. Content Analysis\n",
      "3. Recommendation Engine"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"web-scraping\"></a>\n",
      "# Web Scraping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"crawling-buzzfeed\"></a>\n",
      "## Crawling BuzzFeed\n",
      "\n",
      "The first step in this process is for us to crawl BuzzFeed and collect a large list of URLs that correspond to GIF listicles in the format [TOM FILL THIS IN HERE]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import requests\n",
      "import Queue\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "srcs = [\"shawnkathleen/flying-this-holiday-season-dont-be-one-of-these-iakw\"]\n",
      "\n",
      "def diff(a, b):\n",
      "    \"\"\"Return the different between two lists.\"\"\"\n",
      "    b = set(b)\n",
      "    return [c for c in a if c not in b]\n",
      "\n",
      "def parse_href(url_string):\n",
      "    \"\"\"Parse the URL from an HTML string containing one. If no URL, return False.\"\"\"\n",
      "    href_idx = url_string.find('href=\"')\n",
      "    \n",
      "    if href_idx <= 0:\n",
      "        return False\n",
      "    \n",
      "    partial_string = url_string[href_idx+7:]\n",
      "    return partial_string[0:partial_string.find('\"')]\n",
      "\n",
      "def is_gif_article(gif_article):\n",
      "    \"\"\"Return True if the article is a listicle containing GIFs, False otherwise.\"\"\"\n",
      "    \n",
      "    buzzfeed_article = requests.get(\"http://www.buzzfeed.com/\"+gif_article)\n",
      "    listicle_items = buzzfeed_article.content.split('class=\"buzz_superlist_item')\n",
      "    \n",
      "    for item in listicle_items[1:-1]:\n",
      "        # Check if item contains a GIF.\n",
      "        if item.find('.gif') > 0:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "def find_other_gif_articles(gif_article):\n",
      "    \"\"\"Search 'related articles' listed on input article for possible GIFs.\"\"\"\n",
      "    gif_articles = []\n",
      "    \n",
      "    buzzfeed_article = requests.get(\"http://www.buzzfeed.com/\"+gif_article)\n",
      "    related_strings = buzzfeed_article.content.split('rel:gt_act=\"related-link/name\"')\n",
      "\n",
      "    for related_string in related_strings[1:]:\n",
      "        href = parse_href(related_string)\n",
      "        if is_gif_article(href) > 0:\n",
      "            gif_articles.append(href)\n",
      "            \n",
      "    return gif_articles\n",
      "\n",
      "def crawl_for_gif_pages(source, limit=1000, delay=0.5):\n",
      "    \"\"\"Crawl Buzzfeed, starting at the source, collecting pages that contain GIFs.\"\"\"\n",
      "    found = []\n",
      "\n",
      "    sources = []\n",
      "    sources.append(source)\n",
      "\n",
      "    while (len(found)<limit and len(sources)>0):\n",
      "        source = sources.pop()\n",
      "\n",
      "        new_article_candidates = find_other_gif_articles(source)\n",
      "        new_articles = diff(new_article_candidates, found)\n",
      "\n",
      "        found.extend(new_articles)\n",
      "        sources.extend(new_articles)\n",
      "        \n",
      "        time.sleep(delay)\n",
      "    \n",
      "    return pd.Series(found)\n",
      "\n",
      "# Crawl all of the sources\n",
      "gif_pages = crawl_for_gif_pages(srcs)\n",
      "gif_pages.to_csv('data/buzzfeed_gif_links.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Display all of the GIF\n",
      "gif_pages = pd.read_csv('data/buzzfeed_gif_links.txt')\n",
      "print gif_pages.shape\n",
      "gif_pages.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(203, 1)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>simplysinova/25-mouthwatering-dessert-gifs-k808</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> giulsi/20-amazing-will-ferrells-gifs-for-any-o...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> erinf45e036d0f/34-david-bowie-gifs-for-all-occ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>   simplysinova/25-mouthwatering-dessert-gifs-k808</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> giulsi/20-amazing-will-ferrells-gifs-for-any-o...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> erinf45e036d0f/34-david-bowie-gifs-for-all-occ...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "     simplysinova/25-mouthwatering-dessert-gifs-k808\n",
        "0  giulsi/20-amazing-will-ferrells-gifs-for-any-o...\n",
        "1  erinf45e036d0f/34-david-bowie-gifs-for-all-occ...\n",
        "2    simplysinova/25-mouthwatering-dessert-gifs-k808\n",
        "3  giulsi/20-amazing-will-ferrells-gifs-for-any-o...\n",
        "4  erinf45e036d0f/34-david-bowie-gifs-for-all-occ..."
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"scraping-buzzfeed\"></a>\n",
      "## Scraping BuzzFeed\n",
      "\n",
      "Once we have a list of a large number of BuzzFeed GIF articles, we scraped each of them individually for the article title, each GIF, and each GIF's associated text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def newscrapepage(url):\n",
      "    \"\"\"\n",
      "    Given the URL of a BuzzFeed listicle, return a DataFrame containing the title text,\n",
      "    GIF URL, and GIF text of each item.\n",
      "    \"\"\"\n",
      "    dfs = []\n",
      "    try:\n",
      "        page = urllib2.urlopen(url).read()\n",
      "        soup = BeautifulSoup(page)\n",
      "        soup.prettify()\n",
      "        imgs = []\n",
      "        texts = []\n",
      "        title = \"blank\"\n",
      "        for tag in soup.findAll('img'):\n",
      "            if tag.has_key('src'):\n",
      "                img = tag['src']\n",
      "                if(img[-12:] == \"_preview.gif\"):\n",
      "                    img = img[:-12] + \".gif\"\n",
      "                    imgs.append(img)\n",
      "                    new = tag.findPrevious('h2')\n",
      "                    text = parser.unescape(''.join(new.findAll(text=True)))\n",
      "                    texts.append(text)\n",
      "        for tag in soup.findAll('title'):\n",
      "            title = parser.unescape(''.join(tag.findAll(text=True)))\n",
      "        for i in xrange(min(len(imgs),len(texts))):\n",
      "            dfs.append({'URL' : imgs[i], 'Text' : texts[i], 'Title' : title})\n",
      "        return dfs\n",
      "    except:\n",
      "        print (u\"Could not open page \" + url)\n",
      "        return []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Use the above scraping function on each crawled link\n",
      "to collect GIF information from each page\n",
      "\"\"\"\n",
      "\n",
      "urlfile = open('data/buzzfeed_gif_links.txt','r')\n",
      "pages = {}\n",
      "for line in urlfile:\n",
      "    line = 'http://www.buzzfeed.com/' + line.replace('\\n','')\n",
      "    if not(line in pages):\n",
      "        pages[line] = 1\n",
      "urlfile_new = open('data/buzzfeed_gif_links_pt2.txt','r')\n",
      "for line in urlfile_new:\n",
      "    line = 'http://www.buzzfeed.com/' + line.replace('\\n','')\n",
      "    if not(line in pages):\n",
      "        pages[line] = 1\n",
      "gifs = []\n",
      "for page in pages:\n",
      "    gifs.append(newscrapepage(page))\n",
      "bf = [item for sublist in gifs for item in sublist]\n",
      "df = pd.DataFrame(bf)\n",
      "df.to_csv('data/buzzfeed_gifs_final.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "buzzfeed_gifs = pd.read_csv('data/buzzfeed_gifs_final.csv',encoding='utf-8')\n",
      "buzzfeed_gifs.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0</th>\n",
        "      <th>Text</th>\n",
        "      <th>Title</th>\n",
        "      <th>URL</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> 1. Because I just wasn\u2019t one of those people w...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 2. Because there are SO many options to choose...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2</td>\n",
        "      <td> 3. Some days you hate every subject in the wor...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 3</td>\n",
        "      <td> 4. And other days you\u2019re super ambitious and t...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 4</td>\n",
        "      <td> 5. But either way you know that you don\u2019t know...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "   Unnamed: 0                                               Text  \\\n",
        "0           0  1. Because I just wasn\u2019t one of those people w...   \n",
        "1           1  2. Because there are SO many options to choose...   \n",
        "2           2  3. Some days you hate every subject in the wor...   \n",
        "3           3  4. And other days you\u2019re super ambitious and t...   \n",
        "4           4  5. But either way you know that you don\u2019t know...   \n",
        "\n",
        "                                               Title  \\\n",
        "0  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "1  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "2  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "3  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "4  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "\n",
        "                                                 URL  \n",
        "0  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "1  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "2  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "3  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  \n",
        "4  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  "
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#scraping-giphy\"></a>\n",
      "## Scraping Giphy\n",
      "\n",
      "In case we needed more GIF data, we also built a simple GIF scraper for giphy, a website where people can upload and tag GIFs.\n",
      "\n",
      "Although Giphy tries to keep people from being able to scrape their site, after some exploring we found a trick in their URL encoding that allowed us to query each of the emotion tags in their tag repository and iterate through each page of GIFs.\n",
      "\n",
      "Below, we see the scraper that iterates through each of these emotions and pages and collects about 40,000 GIFs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfs = []\n",
      "emotions = ['angry', 'bored', 'disappointed', 'drunk', 'embarassed', \n",
      "           'excited', 'frustrated', 'happy', 'hungry', 'inspired', \n",
      "           'lonely', 'love', 'nervous', 'pain', 'reaction', 'relaxed', \n",
      "           'sad', 'sassy', 'scared', 'shocked', 'sick', 'stressed', \n",
      "           'surprised', 'suspicious', 'tired', 'unimpressed']\n",
      "\n",
      "for emotion in emotions:\n",
      "    print emotion\n",
      "    urls = ['http://giphy.com/search/' + emotion + '/' + str(x) +'?is=2' for x in range(100)]\n",
      "    for url in urls:\n",
      "        try:\n",
      "            page = urllib2.urlopen(url).read()\n",
      "            soup = BeautifulSoup(page)\n",
      "            images = soup.findAll('div', class_='hoverable-gif')\n",
      "\n",
      "            data = []\n",
      "            for image in images:\n",
      "                tags = [x.text.split('#')[1] for x in image.span.findAll('a')]\n",
      "                url = image.a.figure.img['src'].replace('_s', '')\n",
      "                dfs.append({'tags':tags, 'url':url, 'emotion': emotion})\n",
      "        except:\n",
      "            print 'Done with ' + emotion\n",
      "            break\n",
      "    \n",
      "data = pd.DataFrame(dfs)\n",
      "data.to_csv('data/giphy_emotions_gifs.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "giphy_gifs = pd.read_csv('data/giphy_emotions_gifs.csv')\n",
      "print giphy_gifs.shape\n",
      "giphy_gifs.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(39975, 4)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0</th>\n",
        "      <th>emotion</th>\n",
        "      <th>tags</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td> angry</td>\n",
        "      <td>           [u'angry', u'horse', u'maximus']</td>\n",
        "      <td> http://media2.giphy.com/media/dOZj6qlD8Kwta/20...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> angry</td>\n",
        "      <td>                [u'angry', u'taylor swift']</td>\n",
        "      <td> http://media0.giphy.com/media/ORKQqVjegOvzq/20...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2</td>\n",
        "      <td> angry</td>\n",
        "      <td>          [u'angry', u'coach', u'throwing']</td>\n",
        "      <td> http://media2.giphy.com/media/110gOs75GuUWyY/2...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 3</td>\n",
        "      <td> angry</td>\n",
        "      <td>                  [u'angry', u'frustrated']</td>\n",
        "      <td> http://media0.giphy.com/media/yI9YaxO7OCz5K/20...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 4</td>\n",
        "      <td> angry</td>\n",
        "      <td> [u'angry', u'mario', u'nintendo', u'zoom']</td>\n",
        "      <td> http://media0.giphy.com/media/kLEuIxZJh8VG0/20...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "   Unnamed: 0 emotion                                        tags  \\\n",
        "0           0   angry            [u'angry', u'horse', u'maximus']   \n",
        "1           1   angry                 [u'angry', u'taylor swift']   \n",
        "2           2   angry           [u'angry', u'coach', u'throwing']   \n",
        "3           3   angry                   [u'angry', u'frustrated']   \n",
        "4           4   angry  [u'angry', u'mario', u'nintendo', u'zoom']   \n",
        "\n",
        "                                                 url  \n",
        "0  http://media2.giphy.com/media/dOZj6qlD8Kwta/20...  \n",
        "1  http://media0.giphy.com/media/ORKQqVjegOvzq/20...  \n",
        "2  http://media2.giphy.com/media/110gOs75GuUWyY/2...  \n",
        "3  http://media0.giphy.com/media/yI9YaxO7OCz5K/20...  \n",
        "4  http://media0.giphy.com/media/kLEuIxZJh8VG0/20...  "
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"crowdsourced-tagging\"></a>\n",
      "# Crowdsourced GIF Sentiment Tagging\n",
      "\n",
      "Once we had a large corpus of GIFs and associated text and titles from our BuzzFeed crawling and scraping, we knew that we needed to build some type of dataset that associated each GIF with some type of emotional component. The objective in mind is that in order to match text to a GIF, we must find the emotional content of the text, and find a GIF with corresponding emotional content.\n",
      "\n",
      "We decided to do this by crowdsourcing the association of GIFs to sentiment (henceforth called \"Sentiment Tagging\"). To do this, we created a website where people could classify a random GIF from our corpus of GIFs. Our spectrum of GIF emotions came from the NC State tweet sentiment visualization map, which can be found [here](http://www.csc.ncsu.edu/faculty/healey/tweet_viz/tweet_app/).\n",
      "\n",
      "The horizontal axis is \"valence\", ranging from negative on the left side and positive on the right side. The vertical axis is \"arousal\", or general alertness, with high alertness on top and low alertness on bottom. The full spectrum is shown below. The URL of the crowdsourcing site is [http://tomssilver.com/cs109/](http://tomssilver.com/cs109/).\n",
      "\n",
      "\n",
      "<img src=\"http://tomssilver.com/cs109/images/sentiment_circle.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final version of our crowdsourcing platform, found here: [http://tomssilver.com/cs109/](http://tomssilver.com/cs109/), can be seen below. A user clicks somewhere on the map to place the GIF somewhere on the 2D emotional spectrum."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://tomssilver.com/cs109/images/crowdsourcing_screencap.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The user can also go to the \"GIF Map\" section of the site to see all of the results. In the example below, we have clicked on a dot that has been placed in the \"Excited\" category."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://tomssilver.com/cs109/images/results.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Building the Crowdsourcing Site\n",
      "\n",
      "Building the crowdsourcing site required the programming of a dynamic PHP/MySQL website. We have listed the main PHP and Javascript snippets that provided for the core functionality of the site.\n",
      "\n",
      "\n",
      "The first component is the Javascript that loads the GIFs and handles interaction with the sentiment tagging graph. This code can be seen below:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "$('.gif_point').hide();\n",
      "\n",
      "// Use AJAX to get a GIF of the given gif_id\n",
      "function set_gif(id)\n",
      "{\n",
      "    $.ajax({\n",
      "          url: \"print_gif.php?id=\"+id,\n",
      "          cache: false\n",
      "        })\n",
      "          .done(function(data) {\n",
      "            // Load the GIF into the DOM\n",
      "            $(\"#gif\").html(data);\n",
      "          });\n",
      "}\n",
      "\n",
      "function bind_set_gif(id)\n",
      "{\n",
      "    $(\"#\"+id).on('click', function ()\n",
      "    {\n",
      "        set_gif(id);\n",
      "    });\n",
      "}\n",
      "\n",
      "function update_sizes(id, w, h)\n",
      "{\n",
      "    $(\"#\"+id).css(\"top\", parseInt(h)+\"px\");\n",
      "    $(\"#\"+id).css(\"left\", parseInt(w)+\"px\");\n",
      "\n",
      "    $(\"#\"+id).show();\n",
      "\n",
      "    bind_set_gif(id);\n",
      "}\n",
      "\n",
      "function resize(id)\n",
      "{\n",
      "\n",
      "  offset = $('#sent_circ').offset();\n",
      "  w = parseFloat($('#sent_circ').width());\n",
      "  h = parseFloat($('#sent_circ').height());\n",
      "\n",
      "  l = parseFloat($(\"#\"+id).css('left'));\t\t\t\n",
      "  t = parseFloat($(\"#\"+id).css('top'));\t\n",
      "\n",
      "  update_sizes(id, w*l, t*h);\t\t  \n",
      "\n",
      "\n",
      "}\n",
      "\n",
      "$(document).ready(function() {    \t\n",
      "\n",
      "    $('#done').ready(function() {\n",
      "\n",
      "        $('.gif_point').each(function() {\n",
      "\n",
      "          resize($(this).attr(\"id\"));\n",
      "\n",
      "        });\n",
      "\n",
      "    });\n",
      "});\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we had PHP code that retrieved a random GIF from the MySQL database and returned appropriate HTML to be injected into the DOM."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "<?\n",
      "\trequire_once(\"connect.php\");\n",
      "\t$query = \"SELECT * FROM `links` ORDER BY RAND() LIMIT 1\";\n",
      "\t$result = mysql_query($query);\n",
      "\n",
      "\twhile ($row = mysql_fetch_array($result)) {\n",
      "    \t$gif_id = $row['gif_id'];\n",
      "\t\t$gif_link = $row['gif_link'];\n",
      "\t\tbreak;\n",
      "\t}\n",
      "\t\n",
      "?>\t\n",
      "\n",
      "<input type=\"hidden\" id=\"gif_id\" value=\"<? echo $gif_id; ?>\">\n",
      "<img class=\"img-responsive\" src=\"<? echo $gif_link; ?>\" alt=\"\">\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of couse, we also have PHP that retrieves a GIF of a specified `gif_id` from the MySQL database and returns appropriate HTML to be injected in the DOM."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "<?\n",
      "\trequire_once(\"connect.php\");\n",
      "\t$query = \"SELECT * FROM `links` WHERE gif_id = \".mysql_real_escape_string($_GET['id']);;\n",
      "\t$result = mysql_query($query);\n",
      "\n",
      "\twhile ($row = mysql_fetch_array($result)) {\n",
      "\t\t$gif_link = $row['gif_link'];\n",
      "\t\tbreak;\n",
      "\t}\n",
      "\t\n",
      "?>\t\n",
      "\n",
      "<img class=\"img-responsive\" src=\"<? echo $gif_link; ?>\" alt=\"\">\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also have PHP code that logs a GIF's `x` and `y` data when a user clicks on the map:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "<?php\n",
      "\trequire_once(\"connect.php\");\n",
      "\n",
      "\t$gif_id = mysql_real_escape_string($_GET['gif_id']);\n",
      "\t$x = mysql_real_escape_string($_GET['x']);\n",
      "\t$y = mysql_real_escape_string($_GET['y']);\n",
      "\n",
      "\tmysql_query(\"INSERT INTO gifs (gif_id, x, y) VALUES ('\".$gif_id.\"', '\".$x.\"', '\".$y.\"')\");\n",
      "\n",
      "?>\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, for the results page, we have code that loads all of the GIF tags as tagged by the users into a single array, and then inject them into the DOM."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "<?\n",
      "\n",
      "\trequire_once(\"connect.php\");\n",
      "\t\n",
      "\t$query = \"SELECT gif_id, AVG(x), AVG(y) FROM `gifs` GROUP BY gif_id\";\n",
      "\t$result = mysql_query($query);\n",
      "\t\n",
      "\t$gif_array = Array();\n",
      "\t\n",
      "\twhile ($row = mysql_fetch_array($result)) {\n",
      "    \tarray_push($gif_array, $row);\n",
      "\t}\n",
      "\n",
      "?>\n",
      "\n",
      "<? \n",
      "    foreach ($gif_array as $row) {\n",
      "        echo '<div id=\"'.$row[0].'\" class=\"gif_point\" style=\"left: '.floatval($row[1]).'px; top: '.floatval($row[2]).'px;\"></div>';\n",
      "    }\n",
      "    echo \"<div id='done' style='display: hidden;'></div>\";\n",
      "?>\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}