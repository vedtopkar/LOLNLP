{
 "metadata": {
  "name": "",
  "signature": "sha256:8501677638d1ee54a40b529c53885feea429117ad42fa465ac4a8218bbb16c9b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Sentiment Analysis\n",
      "\n",
      "Once we had all of our crowdsourced data that associates GIFs with emotions on the spectrum, we set out to create a sentiment analysis algorithm that can take an input string and associate it with an emotion. The first step is parts of speech tagging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from IPython import display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data1 = pd.read_csv('data/pos_tagged_gif_data.csv')\n",
      "data2 = pd.read_csv('data/categorized_emotions.csv')\n",
      "data = data1.merge(data2, left_on='URL', right_on='url')\n",
      "data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0_x</th>\n",
        "      <th>Unnamed: 0.1</th>\n",
        "      <th>Text</th>\n",
        "      <th>Title</th>\n",
        "      <th>URL</th>\n",
        "      <th>text verbs</th>\n",
        "      <th>text adverbs</th>\n",
        "      <th>text adjectives</th>\n",
        "      <th>text nouns</th>\n",
        "      <th>all text tags</th>\n",
        "      <th>...</th>\n",
        "      <th>title nouns</th>\n",
        "      <th>all title tags</th>\n",
        "      <th>Unnamed: 0_y</th>\n",
        "      <th>id</th>\n",
        "      <th>gif_id</th>\n",
        "      <th>x</th>\n",
        "      <th>y</th>\n",
        "      <th>url</th>\n",
        "      <th>strength</th>\n",
        "      <th>emotion</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  2</td>\n",
        "      <td>  2</td>\n",
        "      <td> 3. Some days you hate every subject in the wor...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td>           ['hate', 'decide', 'destined']</td>\n",
        "      <td>         ['just']</td>\n",
        "      <td>                            ['you\\xe2\\x80\\x99re']</td>\n",
        "      <td>           ['days', 'subject', 'world', 'failure']</td>\n",
        "      <td> [('3.', 'CD'), ('Some', 'DT'), ('days', 'NNS')...</td>\n",
        "      <td>...</td>\n",
        "      <td> ['Reasons', 'Choosing', 'College', 'Major', 'I...</td>\n",
        "      <td> [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...</td>\n",
        "      <td>  830</td>\n",
        "      <td>  714</td>\n",
        "      <td>  4</td>\n",
        "      <td>-0.418667</td>\n",
        "      <td>-0.142857</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td> 0.442369</td>\n",
        "      <td>       sad</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  3</td>\n",
        "      <td>  3</td>\n",
        "      <td> 4. And other days you\u2019re super ambitious and t...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td>                          ['think', 'do']</td>\n",
        "      <td>               []</td>\n",
        "      <td>      ['other', 'you\\xe2\\x80\\x99re', 'ambitious']</td>\n",
        "      <td>          ['days', 'super', 'everything', 'world']</td>\n",
        "      <td> [('4.', 'CD'), ('And', 'CC'), ('other', 'JJ'),...</td>\n",
        "      <td>...</td>\n",
        "      <td> ['Reasons', 'Choosing', 'College', 'Major', 'I...</td>\n",
        "      <td> [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...</td>\n",
        "      <td> 1215</td>\n",
        "      <td> 1038</td>\n",
        "      <td>  5</td>\n",
        "      <td>-0.144000</td>\n",
        "      <td>-0.169173</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td> 0.222161</td>\n",
        "      <td> depressed</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  7</td>\n",
        "      <td>  7</td>\n",
        "      <td> 8. Every time you speak to an adult they\u2019re li...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td> ['speak', 'supposed', 'have', 'figured']</td>\n",
        "      <td>   ['out', 'now']</td>\n",
        "      <td> ['\\xe2\\x80\\x9cso', 'major', 'you\\xe2\\x80\\x99re']</td>\n",
        "      <td> ['Every', 'time', 'adult', 'they\\xe2\\x80\\x99re...</td>\n",
        "      <td> [('8.', 'CD'), ('Every', 'NNP'), ('time', 'NN'...</td>\n",
        "      <td>...</td>\n",
        "      <td> ['Reasons', 'Choosing', 'College', 'Major', 'I...</td>\n",
        "      <td> [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...</td>\n",
        "      <td>  292</td>\n",
        "      <td>  319</td>\n",
        "      <td>  9</td>\n",
        "      <td> 0.148611</td>\n",
        "      <td> 0.023438</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td> 0.150448</td>\n",
        "      <td>  pleasant</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  7</td>\n",
        "      <td>  7</td>\n",
        "      <td> 8. Every time you speak to an adult they\u2019re li...</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td> ['speak', 'supposed', 'have', 'figured']</td>\n",
        "      <td>   ['out', 'now']</td>\n",
        "      <td> ['\\xe2\\x80\\x9cso', 'major', 'you\\xe2\\x80\\x99re']</td>\n",
        "      <td> ['Every', 'time', 'adult', 'they\\xe2\\x80\\x99re...</td>\n",
        "      <td> [('8.', 'CD'), ('Every', 'NNP'), ('time', 'NN'...</td>\n",
        "      <td>...</td>\n",
        "      <td> ['Reasons', 'Choosing', 'College', 'Major', 'I...</td>\n",
        "      <td> [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...</td>\n",
        "      <td>  293</td>\n",
        "      <td> 2611</td>\n",
        "      <td>  9</td>\n",
        "      <td>-0.355172</td>\n",
        "      <td>-0.199029</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td> 0.407136</td>\n",
        "      <td>   unhappy</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 14</td>\n",
        "      <td> 14</td>\n",
        "      <td> 15. And you\u2019re just like, \u201cUhhhh well okay then.\u201d</td>\n",
        "      <td> 18 Reasons Why Choosing A College Major Is Imp...</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td>                                       []</td>\n",
        "      <td> ['just', 'well']</td>\n",
        "      <td>       ['you\\xe2\\x80\\x99re', '\\xe2\\x80\\x9cUhhhh']</td>\n",
        "      <td>                                          ['okay']</td>\n",
        "      <td> [('15.', 'CD'), ('And', 'CC'), ('you\\xe2\\x80\\x...</td>\n",
        "      <td>...</td>\n",
        "      <td> ['Reasons', 'Choosing', 'College', 'Major', 'I...</td>\n",
        "      <td> [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...</td>\n",
        "      <td>  981</td>\n",
        "      <td>  842</td>\n",
        "      <td> 16</td>\n",
        "      <td> 0.434360</td>\n",
        "      <td> 0.125571</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/2014-05/enhanc...</td>\n",
        "      <td> 0.452147</td>\n",
        "      <td>     happy</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 23 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "   Unnamed: 0_x  Unnamed: 0.1  \\\n",
        "0             2             2   \n",
        "1             3             3   \n",
        "2             7             7   \n",
        "3             7             7   \n",
        "4            14            14   \n",
        "\n",
        "                                                Text  \\\n",
        "0  3. Some days you hate every subject in the wor...   \n",
        "1  4. And other days you\u2019re super ambitious and t...   \n",
        "2  8. Every time you speak to an adult they\u2019re li...   \n",
        "3  8. Every time you speak to an adult they\u2019re li...   \n",
        "4  15. And you\u2019re just like, \u201cUhhhh well okay then.\u201d   \n",
        "\n",
        "                                               Title  \\\n",
        "0  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "1  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "2  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "3  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "4  18 Reasons Why Choosing A College Major Is Imp...   \n",
        "\n",
        "                                                 URL  \\\n",
        "0  http://s3-ec.buzzfed.com/static/2014-05/enhanc...   \n",
        "1  http://s3-ec.buzzfed.com/static/2014-05/enhanc...   \n",
        "2  http://s3-ec.buzzfed.com/static/2014-05/enhanc...   \n",
        "3  http://s3-ec.buzzfed.com/static/2014-05/enhanc...   \n",
        "4  http://s3-ec.buzzfed.com/static/2014-05/enhanc...   \n",
        "\n",
        "                                 text verbs      text adverbs  \\\n",
        "0            ['hate', 'decide', 'destined']          ['just']   \n",
        "1                           ['think', 'do']                []   \n",
        "2  ['speak', 'supposed', 'have', 'figured']    ['out', 'now']   \n",
        "3  ['speak', 'supposed', 'have', 'figured']    ['out', 'now']   \n",
        "4                                        []  ['just', 'well']   \n",
        "\n",
        "                                    text adjectives  \\\n",
        "0                             ['you\\xe2\\x80\\x99re']   \n",
        "1       ['other', 'you\\xe2\\x80\\x99re', 'ambitious']   \n",
        "2  ['\\xe2\\x80\\x9cso', 'major', 'you\\xe2\\x80\\x99re']   \n",
        "3  ['\\xe2\\x80\\x9cso', 'major', 'you\\xe2\\x80\\x99re']   \n",
        "4        ['you\\xe2\\x80\\x99re', '\\xe2\\x80\\x9cUhhhh']   \n",
        "\n",
        "                                          text nouns  \\\n",
        "0            ['days', 'subject', 'world', 'failure']   \n",
        "1           ['days', 'super', 'everything', 'world']   \n",
        "2  ['Every', 'time', 'adult', 'they\\xe2\\x80\\x99re...   \n",
        "3  ['Every', 'time', 'adult', 'they\\xe2\\x80\\x99re...   \n",
        "4                                           ['okay']   \n",
        "\n",
        "                                       all text tags  \\\n",
        "0  [('3.', 'CD'), ('Some', 'DT'), ('days', 'NNS')...   \n",
        "1  [('4.', 'CD'), ('And', 'CC'), ('other', 'JJ'),...   \n",
        "2  [('8.', 'CD'), ('Every', 'NNP'), ('time', 'NN'...   \n",
        "3  [('8.', 'CD'), ('Every', 'NNP'), ('time', 'NN'...   \n",
        "4  [('15.', 'CD'), ('And', 'CC'), ('you\\xe2\\x80\\x...   \n",
        "\n",
        "                         ...                          \\\n",
        "0                        ...                           \n",
        "1                        ...                           \n",
        "2                        ...                           \n",
        "3                        ...                           \n",
        "4                        ...                           \n",
        "\n",
        "                                         title nouns  \\\n",
        "0  ['Reasons', 'Choosing', 'College', 'Major', 'I...   \n",
        "1  ['Reasons', 'Choosing', 'College', 'Major', 'I...   \n",
        "2  ['Reasons', 'Choosing', 'College', 'Major', 'I...   \n",
        "3  ['Reasons', 'Choosing', 'College', 'Major', 'I...   \n",
        "4  ['Reasons', 'Choosing', 'College', 'Major', 'I...   \n",
        "\n",
        "                                      all title tags Unnamed: 0_y    id  \\\n",
        "0  [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...          830   714   \n",
        "1  [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...         1215  1038   \n",
        "2  [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...          292   319   \n",
        "3  [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...          293  2611   \n",
        "4  [('18', 'CD'), ('Reasons', 'NNS'), ('Why', 'WR...          981   842   \n",
        "\n",
        "  gif_id         x         y  \\\n",
        "0      4 -0.418667 -0.142857   \n",
        "1      5 -0.144000 -0.169173   \n",
        "2      9  0.148611  0.023438   \n",
        "3      9 -0.355172 -0.199029   \n",
        "4     16  0.434360  0.125571   \n",
        "\n",
        "                                                 url  strength    emotion  \n",
        "0  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  0.442369        sad  \n",
        "1  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  0.222161  depressed  \n",
        "2  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  0.150448   pleasant  \n",
        "3  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  0.407136    unhappy  \n",
        "4  http://s3-ec.buzzfed.com/static/2014-05/enhanc...  0.452147      happy  \n",
        "\n",
        "[5 rows x 23 columns]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training Classifier(s)\n",
      "\n",
      "When it comes to classifying text, binary classification schemas like Naive Bayes or Support Vector Machines (SVM) do pretty well. In order to adapt this to our schema of classifying text into one of the emotional categories, we decided to make $N$ classifiers, classifying text into or not into each of the $N$ emotional categories. Then, once these classifiers are trained, we can put an input string through each of the classifiers and aggregate the predictions to place the input string into one of the emotional categories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data['Text'].astype(str)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "0     3. Some days you hate every subject in the wor...\n",
        "1     4. And other days you\u2019re super ambitious and t...\n",
        "2     8. Every time you speak to an adult they\u2019re li...\n",
        "3     8. Every time you speak to an adult they\u2019re li...\n",
        "4     15. And you\u2019re just like, \u201cUhhhh well okay then.\u201d\n",
        "5          16. And still, nothing seems to be clicking.\n",
        "6     And Now For 8 \"Mass Effect\" Characters Dancing...\n",
        "7                    Your link was successfully shared!\n",
        "8                    Your link was successfully shared!\n",
        "9                                             More Buzz\n",
        "10    Note: once you save these links, they will no ...\n",
        "11                                                  nan\n",
        "12    3. Why does the old lady shoot her house to pi...\n",
        "13     4. Why does the rats\u2019 escape plan involve boats?\n",
        "14    5. Why does Remy care so much about how the re...\n",
        "...\n",
        "3985    13. Irate customers have called you a \u201closer w...\n",
        "3986        16. Holiday circulars make you want to vomit.\n",
        "3987        16. Holiday circulars make you want to vomit.\n",
        "3988           1. Cats Who Will Help You Write Your Novel\n",
        "3989           1. Cats Who Will Help You Write Your Novel\n",
        "3990              4. Does Badger ever become a TV writer?\n",
        "3991    5. Where did the \u201cHello Kitty\u201d phone case come...\n",
        "3992           6. Did Hank ever finish \u201cLeaves of Grass?\u201d\n",
        "3993    10. Did anyone ever try the table-side guacamole?\n",
        "3994                          2. 2. Men are often idiots.\n",
        "3995      3. 3. Sometimes all you have to do is be there.\n",
        "3996                                  6. 6. Be ambitious.\n",
        "3997            8. 8. And just to be safe, even a Plan C.\n",
        "3998                12. 12. Always stick up for yourself.\n",
        "3999              15. 15. It\u2019s cool to have an alter ego.\n",
        "Name: Text, Length: 4000, dtype: object"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "count_vect = CountVectorizer()\n",
      "\n",
      "verbs = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['text verbs']]\n",
      "nouns = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['text nouns']]\n",
      "adverbs = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['text adverbs']]\n",
      "adjectives = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['text adjectives']]\n",
      "\n",
      "title_verbs = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['title verbs']]\n",
      "title_nouns = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['title nouns']]\n",
      "title_adverbs = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['title adverbs']]\n",
      "title_adjectives = [x.replace('[', '').replace(\"'\", '').replace(',', '').replace(']', '') for x in data['title adjectives']]\n",
      "\n",
      "X = [verbs[i] + ' ' + nouns[i] + ' ' + title_verbs[i] + ' ' + title_nouns[i] + ' ' + title_adjectives[i] + ' ' + title_adverbs[i] + ' ' + adverbs[i] + ' ' + adjectives[i] for i in range(len(verbs))]\n",
      "print X[:10]\n",
      "\n",
      "X_train_counts = count_vect.fit_transform(X)\n",
      "X_train_counts.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['hate decide destined days subject world failure  Reasons Choosing College Major Is Impossible   just you\\\\xe2\\\\x80\\\\x99re', 'think do days super everything world  Reasons Choosing College Major Is Impossible    other you\\\\xe2\\\\x80\\\\x99re ambitious', 'speak supposed have figured Every time adult they\\\\xe2\\\\x80\\\\x99re what\\\\xe2\\\\x80\\\\x99s  Reasons Choosing College Major Is Impossible   out now \\\\xe2\\\\x80\\\\x9cso major you\\\\xe2\\\\x80\\\\x99re', 'speak supposed have figured Every time adult they\\\\xe2\\\\x80\\\\x99re what\\\\xe2\\\\x80\\\\x99s  Reasons Choosing College Major Is Impossible   out now \\\\xe2\\\\x80\\\\x9cso major you\\\\xe2\\\\x80\\\\x99re', ' okay  Reasons Choosing College Major Is Impossible   just well you\\\\xe2\\\\x80\\\\x99re \\\\xe2\\\\x80\\\\x9cUhhhh', 'seems be clicking nothing  Reasons Choosing College Major Is Impossible   still ', ' For Mass Effect Characters Dancing Like No One Watching  For Mass Effect Characters Dancing Like No One Watching  Now Now ', 'was shared link  For Mass Effect Characters Dancing Like No One Watching  Now successfully ', 'was shared link  For Mass Effect Characters Dancing Like No One Watching  Now successfully ', ' Buzz  For Mass Effect Characters Dancing Like No One Watching  Now  More']\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 329,
       "text": [
        "(4000, 5487)"
       ]
      }
     ],
     "prompt_number": 329
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import SGDClassifier \n",
      "Y = np.asarray(data['emotion'], dtype=\"|S\")\n",
      "clf = SGDClassifier(n_iter=100)\n",
      "clf.fit(X_train_counts, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 330,
       "text": [
        "SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,\n",
        "       fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
        "       loss='hinge', n_iter=100, n_jobs=1, penalty='l2', power_t=0.5,\n",
        "       random_state=None, rho=None, shuffle=False, verbose=0,\n",
        "       warm_start=False)"
       ]
      }
     ],
     "prompt_number": 330
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_relavent_gif(text):\n",
      "    doc = count_vect.transform([text])\n",
      "    predicted = clf.predict(doc)[0]\n",
      "    print 'Predicted emotion: ' + str(predicted)\n",
      "    \n",
      "    filtered_urls = data[data['emotion'] == predicted]['url']\n",
      "    i = np.random.choice(filtered_urls.index, 1)[0]\n",
      "    url= filtered_urls.ix[i]\n",
      "    display.display(display.Image(url=url))\n",
      "\n",
      "get_relavent_gif('these are nice lights')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicted emotion: relaxed\n"
       ]
      },
      {
       "html": [
        "<img src=\"http://s3-ec.buzzfed.com/static/2014-08/7/23/enhanced/webdr05/anigif_enhanced-28202-1407467364-1.gif\"/>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Image at 0x10ff114d0>"
       ]
      }
     ],
     "prompt_number": 340
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.corpus\n",
      "import nltk.tokenize.punkt\n",
      "import string\n",
      "import re\n",
      " \n",
      "# Create a set of stopwords based on the default NLTK english stopwords\n",
      "# Be sure to also include punctuation and the empty string\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "stopwords.extend(string.punctuation)\n",
      "stopwords.append('')\n",
      "\n",
      "# Instantiate a tokenizer that tokenizes a given string\n",
      "# Also instantiate the NLTK stemmer\n",
      "tokenizer = nltk.tokenize.punkt.PunktWordTokenizer()\n",
      "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
      " \n",
      "def check_matching_tokens(a, b):\n",
      "    \"\"\"\n",
      "    A fuzzy matcher that sees if significant word stems match between two strings.\n",
      "    Inspired by the examples here: http://bommaritollc.com/2014/06/fuzzy-match-sentences-python/\n",
      "    \"\"\"\n",
      "    \n",
      "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(a) if token.lower().strip(string.punctuation) not in stopwords]\n",
      "    stems_a = [stemmer.stem(token) for token in tokens_a]\n",
      "    \n",
      "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer.tokenize(b) if token.lower().strip(string.punctuation) not in stopwords]\n",
      "    stems_b = [stemmer.stem(token) for token in tokens_b]\n",
      "    \n",
      "    match = set(stems_a) & set(stems_b)\n",
      "    return len(match)\n",
      "\n",
      "\n",
      "def find_matching_gifs_by_content(input_string, data=data, output=True):\n",
      "    \"\"\"\n",
      "    Given an input string and a master dataframe (data)\n",
      "    Find the GIFs with matching tokenized content stems (ordered by number of matches)\n",
      "    \"\"\"\n",
      "    \n",
      "    matches = []\n",
      "    \n",
      "    # Iterate over each text and\n",
      "    texts = data['Text'].fillna('')\n",
      "    titles = data['Title'].fillna('')\n",
      "    \n",
      "    combined_text = [texts[i] + ' ' + titles[i] for i in range(len(data))]\n",
      "    for text in combined_text:\n",
      "        text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
      "        n_matches = check_matching_tokens(text, input_string)\n",
      "        matches.append({'Match': True if n_matches > 0 else False, 'n': n_matches })\n",
      "\n",
      "    match_table = pd.DataFrame(matches)\n",
      "    df = data.copy()\n",
      "    df['Match'] = match_table['Match']\n",
      "    df['N Matches'] = match_table['n']\n",
      "    \n",
      "    if np.sum(df['Match']) > 0:\n",
      "        df = df[match_table['Match']].sort(columns='N Matches', ascending=False)\n",
      "        max_n = max(df['N Matches'])\n",
      "        df = df[df['N Matches'] == max_n]\n",
      "        i = np.random.choice(df.index, 1)[0]\n",
      "        url= df.ix[i]['url']\n",
      "        t = df.ix[i]['Text']\n",
      "        if output:\n",
      "            print t, max_n\n",
      "            display.display(display.Image(url=url))\n",
      "        return df\n",
      "    else:\n",
      "        return []\n",
      "    \n",
      "find_matching_gifs_by_content('snow corgi')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41. The snow bun\u2014er\u2014corgi. 2\n"
       ]
      },
      {
       "html": [
        "<img src=\"http://s3-ec.buzzfed.com/static/enhanced/webdr03/2013/8/20/14/anigif_enhanced-buzz-24903-1377025136-17.gif\"/>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Image at 0x110dbce90>"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unnamed: 0_x</th>\n",
        "      <th>Unnamed: 0.1</th>\n",
        "      <th>Text</th>\n",
        "      <th>Title</th>\n",
        "      <th>URL</th>\n",
        "      <th>text verbs</th>\n",
        "      <th>text adverbs</th>\n",
        "      <th>text adjectives</th>\n",
        "      <th>text nouns</th>\n",
        "      <th>all text tags</th>\n",
        "      <th>...</th>\n",
        "      <th>Unnamed: 0_y</th>\n",
        "      <th>id</th>\n",
        "      <th>gif_id</th>\n",
        "      <th>x</th>\n",
        "      <th>y</th>\n",
        "      <th>url</th>\n",
        "      <th>strength</th>\n",
        "      <th>emotion</th>\n",
        "      <th>Match</th>\n",
        "      <th>N Matches</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>3000</th>\n",
        "      <td> 6174</td>\n",
        "      <td> 6174</td>\n",
        "      <td> 41. The snow bun\u2014er\u2014corgi.</td>\n",
        "      <td> 51 Corgi GIFs That Will Change Your Life</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/enhanced/webdr...</td>\n",
        "      <td> []</td>\n",
        "      <td> []</td>\n",
        "      <td> []</td>\n",
        "      <td> ['snow', 'bun\\xe2\\x80\\x94er\\xe2\\x80\\x94corgi']</td>\n",
        "      <td> [('41.', 'CD'), ('The', 'DT'), ('snow', 'NN'),...</td>\n",
        "      <td>...</td>\n",
        "      <td> 3147</td>\n",
        "      <td> 2929</td>\n",
        "      <td> 6176</td>\n",
        "      <td> 0.277333</td>\n",
        "      <td>-0.37218</td>\n",
        "      <td> http://s3-ec.buzzfed.com/static/enhanced/webdr...</td>\n",
        "      <td> 0.464146</td>\n",
        "      <td> relaxed</td>\n",
        "      <td> True</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>1 rows \u00d7 25 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 460,
       "text": [
        "      Unnamed: 0_x  Unnamed: 0.1                        Text  \\\n",
        "3000          6174          6174  41. The snow bun\u2014er\u2014corgi.   \n",
        "\n",
        "                                         Title  \\\n",
        "3000  51 Corgi GIFs That Will Change Your Life   \n",
        "\n",
        "                                                    URL text verbs  \\\n",
        "3000  http://s3-ec.buzzfed.com/static/enhanced/webdr...         []   \n",
        "\n",
        "     text adverbs text adjectives  \\\n",
        "3000           []              []   \n",
        "\n",
        "                                          text nouns  \\\n",
        "3000  ['snow', 'bun\\xe2\\x80\\x94er\\xe2\\x80\\x94corgi']   \n",
        "\n",
        "                                          all text tags  \\\n",
        "3000  [('41.', 'CD'), ('The', 'DT'), ('snow', 'NN'),...   \n",
        "\n",
        "                            ...                         Unnamed: 0_y    id  \\\n",
        "3000                        ...                                 3147  2929   \n",
        "\n",
        "     gif_id         x        y  \\\n",
        "3000   6176  0.277333 -0.37218   \n",
        "\n",
        "                                                    url  strength  emotion  \\\n",
        "3000  http://s3-ec.buzzfed.com/static/enhanced/webdr...  0.464146  relaxed   \n",
        "\n",
        "      Match  N Matches  \n",
        "3000   True          2  \n",
        "\n",
        "[1 rows x 25 columns]"
       ]
      }
     ],
     "prompt_number": 460
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 402
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.sub(r'[^\\x00-\\x7F]+',' ', 'test\\xe2\\x80\\x99')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 376,
       "text": [
        "'test '"
       ]
      }
     ],
     "prompt_number": 376
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}